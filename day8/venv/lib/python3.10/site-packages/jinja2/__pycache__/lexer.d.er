##[pylyzer] failed /home/dmitry/progs/python/WISCourse/Python_assignments/day8/venv/lib/python3.10/site-packages/jinja2/lexer.py 1736025949 29786

.re = pyimport "<failure>"
.<failure> = pyimport "<failure>"

.t = pyimport "<failure>"


.___v_desugar_1 = pyimport "<failure>"

.literal_eval: Never
.___v_desugar_2 = pyimport "<failure>"

.deque: Never
.___v_desugar_3 = pyimport "<failure>"

.intern: Never
.___v_desugar_4 = pyimport "_identifier"
._identifier = pyimport "_identifier"
.name_re: Never
.___v_desugar_5 = pyimport "exceptions"
.exceptions = pyimport "exceptions"
.TemplateSyntaxError: {exceptions.TemplateSyntaxError}
.___v_desugar_6 = pyimport "utils"
.utils = pyimport "utils"
.LRUCache: {utils.LRUCache}

._lexer_cache: Never
.whitespace_re: Never
.newline_re: Never
.string_re: Never
.integer_re: Never
.float_re: Never
.TOKEN_ADD: Never
.TOKEN_ASSIGN: Never
.TOKEN_COLON: Never
.TOKEN_COMMA: Never
.TOKEN_DIV: Never
.TOKEN_DOT: Never
.TOKEN_EQ: Never
.TOKEN_FLOORDIV: Never
.TOKEN_GT: Never
.TOKEN_GTEQ: Never
.TOKEN_LBRACE: Never
.TOKEN_LBRACKET: Never
.TOKEN_LPAREN: Never
.TOKEN_LT: Never
.TOKEN_LTEQ: Never
.TOKEN_MOD: Never
.TOKEN_MUL: Never
.TOKEN_NE: Never
.TOKEN_PIPE: Never
.TOKEN_POW: Never
.TOKEN_RBRACE: Never
.TOKEN_RBRACKET: Never
.TOKEN_RPAREN: Never
.TOKEN_SEMICOLON: Never
.TOKEN_SUB: Never
.TOKEN_TILDE: Never
.TOKEN_WHITESPACE: Never
.TOKEN_FLOAT: Never
.TOKEN_INTEGER: Never
.TOKEN_NAME: Never
.TOKEN_STRING: Never
.TOKEN_OPERATOR: Never
.TOKEN_BLOCK_BEGIN: Never
.TOKEN_BLOCK_END: Never
.TOKEN_VARIABLE_BEGIN: Never
.TOKEN_VARIABLE_END: Never
.TOKEN_RAW_BEGIN: Never
.TOKEN_RAW_END: Never
.TOKEN_COMMENT_BEGIN: Never
.TOKEN_COMMENT_END: Never
.TOKEN_COMMENT: Never
.TOKEN_LINESTATEMENT_BEGIN: Never
.TOKEN_LINESTATEMENT_END: Never
.TOKEN_LINECOMMENT_BEGIN: Never
.TOKEN_LINECOMMENT_END: Never
.TOKEN_LINECOMMENT: Never
.TOKEN_DATA: Never
.TOKEN_INITIAL: Never
.TOKEN_EOF: Never
.operators: Never
.reverse_operators: Never

.operator_re: Never
.ignored_tokens: Never
.ignore_if_empty: Never
._describe_token_type: (token_type: Str) -> Str
.describe_token: (token: Obj) -> Str
.describe_token_expr: (expr: Str) -> Str
.count_newlines: (value: Str) -> Int
.compile_rules: Never
.Failure: ClassType
.Failure.message: Str
.Failure.error_class: Never
.Failure.__call__: Never
.Failure.__call__: Never

.Token: ClassType
.Token <: Never
.Token.Type: Str
.Token.lineno: Int
.Token.value: Str
.Token.__str__: (self: lexer.Token) -> Str
.Token.test: (self: lexer.Token, expr: Str) -> Bool
.Token.test_any: (self: lexer.Token, *iterable: Str) -> Bool

.TokenStreamIterator: ClassType
.TokenStreamIterator.__call__: Never
.TokenStreamIterator.__iter__: (self: lexer.Failure) -> Never
.TokenStreamIterator.__next__: (self: lexer.Failure) -> lexer.Token

.TokenStream: ClassType
.TokenStream.current: Never
.TokenStream._pushed: Never
.TokenStream.name: Never
.TokenStream.filename: Never
.TokenStream._iter: Never
.TokenStream.closed: Never
.TokenStream.__call__: (generator: Obj, name: Obj, filename: Obj) -> lexer.TokenStream
.TokenStream.__iter__: (self: lexer.TokenStream) -> Never
.TokenStream.__bool__: (self: lexer.TokenStream) -> Bool
.TokenStream.eos: (self: lexer.TokenStream) -> Bool
.TokenStream.push: (self: lexer.TokenStream, token: lexer.Token) -> NoneType
.TokenStream.look: (self: lexer.TokenStream) -> lexer.Token
.TokenStream.skip: (self: lexer.TokenStream, n: Int := Int) -> NoneType
.TokenStream.next_if: (self: Never, expr: Str) -> {None}
.TokenStream.skip_if: (self: lexer.TokenStream, expr: Str) -> Bool
.TokenStream.__next__: (self: lexer.TokenStream) -> lexer.Token
.TokenStream.close: (self: lexer.TokenStream) -> NoneType
.TokenStream.expect: (self: lexer.TokenStream, expr: Str) -> lexer.Token

.get_lexer: (environment: Obj) -> Never
.OptionalLStrip: ClassType
.OptionalLStrip <: global::GenericTuple
.OptionalLStrip.__slots__: global::Tuple([])
.OptionalLStrip.__new__: (cls: Obj, *members: Obj, **kwargs := ?56150) -> Never

.Type__Rule: ClassType
.Type__Rule <: Never
.Type__Rule.pattern: Never
.Type__Rule.tokens: Never
.Type__Rule.command: Never

.Lexer: ClassType
.Lexer.keep_trailing_newline: Never
.Lexer.newline_sequence: Never
.Lexer.lstrip_blocks: Never
.Lexer.rules: Never
.Lexer.__call__: (environment: Obj) -> lexer.Lexer
.Lexer._normalize_newlines: (self: lexer.Lexer, value: Str) -> Str
.Lexer.tokenize: (self: lexer.Lexer, source: Str, name: Obj := Obj, filename: Obj := Obj, state: Obj := Obj) -> lexer.TokenStream
.Lexer.wrap: (self: lexer.Lexer, stream: global::Iterable(global::Indexable(Obj, Str)), name: Obj := Obj, filename: Obj := Obj) -> NoneType
.Lexer.tokeniter: (self: lexer.Lexer, source: Str, name: Obj, filename: Obj := Obj, state: Obj := Obj) -> global::Iterable(global::Indexable(Obj, Str))

